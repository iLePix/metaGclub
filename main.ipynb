{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current Directory: {current_dir}\")\n",
    "\n",
    "path = os.path.join(current_dir, 'res/Source_Data_24Oct2022.xlsx')\n",
    "print(f\"Current Directory: {path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all sheets\n",
    "dfs = pd.read_excel(path, sheet_name=None)\n",
    "\n",
    "# Now dfs is a dictionary where the keys are the sheet names\n",
    "for sheet_name, df in dfs.items():\n",
    "    print(f\"Sheet name: {sheet_name}\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process 'subject_metadata' DataFrame\n",
    "subject_metadata = dfs['subject_metadata']\n",
    "subject_metadata = subject_metadata.set_index('sample_name')\n",
    "\n",
    "# Function to process DataFrames where samples are columns\n",
    "def process_df(df, feature_name):\n",
    "    df = df.set_index(feature_name).transpose()\n",
    "    df.index.name = 'sample_name'\n",
    "    return df\n",
    "\n",
    "# Process other DataFrames\n",
    "metaphlan_counts_T = process_df(dfs['metaphlan_counts'], 'clade_name')\n",
    "metaphlan_rel_ab_T = process_df(dfs['metaphlan_rel_ab'], 'clade_name')\n",
    "humann_KO_group_counts_T = process_df(dfs['humann_KO_group_counts'], 'Gene Family')\n",
    "humann_pathway_counts_T = process_df(dfs['humann_pathway_counts'], 'Pathway')\n",
    "\n",
    "# List of DataFrames to merge\n",
    "data_frames = [\n",
    "    subject_metadata,\n",
    "    metaphlan_counts_T,\n",
    "    metaphlan_rel_ab_T,\n",
    "    humann_KO_group_counts_T,\n",
    "    humann_pathway_counts_T\n",
    "]\n",
    "\n",
    "# Merge all DataFrames on 'sample_name' index\n",
    "merged_df = reduce(\n",
    "    lambda left, right: pd.merge(left, right, left_index=True, right_index=True, how='inner'),\n",
    "    data_frames\n",
    ")\n",
    "\n",
    "tpot_df = merged_df.copy()\n",
    "\n",
    "# Display the merged DataFrame\n",
    "merged_df.to_csv('source_data_24oct2022.csv', index=False)\n",
    "print(merged_df.head())\n",
    "# Access data for a specific patient (e.g., 'DP644')\n",
    "#patient_data = merged_df.loc['DP644']\n",
    "#print(patient_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Separate features and target variable\n",
    "X = merged_df.drop('Case_status', axis=1)\n",
    "y = merged_df['Case_status']\n",
    "\n",
    "# Step 2: One-hot encode categorical columns\n",
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "# Step 3: Impute missing values\n",
    "imputer = SimpleImputer(strategy=\"mean\")  # You can also use 'median' or 'most_frequent'\n",
    "X_imputed = imputer.fit_transform(X_encoded)\n",
    "\n",
    "# Step 4: Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# Step 5: Apply PCA\n",
    "pca = PCA(n_components=512)\n",
    "X_pca = pca.fit_transform(X_standardized)\n",
    "# Step 6: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 7: Train the Model (Random Forest Classifier in this case)\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Step 8: Make Predictions and Evaluate the Model\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 1: Separate features and target\n",
    "tpot_df = tpot_df.copy()  # Make a copy to avoid modifying the original DataFrame\n",
    "X = tpot_df.drop('Case_status', axis=1)\n",
    "y = tpot_df['Case_status']\n",
    "\n",
    "# Step 2: Identify numeric and categorical columns\n",
    "numeric_cols = X.select_dtypes(include=['number']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Step 3: Impute missing values\n",
    "# Impute numeric columns with the mean\n",
    "numeric_imputer = SimpleImputer(strategy='mean')\n",
    "X[numeric_cols] = numeric_imputer.fit_transform(X[numeric_cols])\n",
    "\n",
    "# Impute categorical columns with the most frequent value\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "X[categorical_cols] = categorical_imputer.fit_transform(X[categorical_cols])\n",
    "\n",
    "# Step 4: Encode categorical variables (One-Hot Encoding or Label Encoding)\n",
    "# For simplicity, let's use One-Hot Encoding for categorical variables\n",
    "X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Step 5: Encode target labels (if not already numeric)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)  # Convert 'Control'/'PD' to 0/1\n",
    "\n",
    "# Step 6: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 7: TPOT setup and training\n",
    "tpot = TPOTClassifier(generations=5, population_size=20, verbosity=2, random_state=42, scoring='f1')\n",
    "\n",
    "# Fit TPOT to training data\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate TPOT on test data\n",
    "print(f\"TPOT Test Accuracy: {tpot.score(X_test, y_test)}\")\n",
    "\n",
    "# Export the best pipeline\n",
    "tpot.export('best_pipeline.py')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
